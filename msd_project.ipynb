{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As always, we import everything\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import hdf5_getters as getters\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords as stop_words\n",
    "from textblob import Word\n",
    "import pycountry\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from gensim import corpora, models\n",
    "import matplotlib\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our project consists of exploring the lyrics of many songs and finding themes and the usage of the words used in these songs over time. We use the Million Song dataset to find information about the song as well as various other datasets and sources to find lyrics data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection and Descriptive Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by getting a list of all the files from our dataset. The Million Song dataset organises the dataset in multiple files and directories. The following code snippet gets all these files and prints the number of the files found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './' + 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all_files contain all the files in the Million song subset downloaded (10000 datapoints)\n",
    "all_files = []\n",
    "for (dirpath, dirnames, filenames) in os.walk(os.path.join(data_dir + \"/MillionSongSubset/data\")):\n",
    "    all_files.extend([dirpath + \"/\" + filename for filename in filenames if filename.endswith(\".h5\")])\n",
    "all_files_num = len(all_files)\n",
    "all_files_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helpful methods to get information from million song dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_songs(filename):\n",
    "    \"\"\"\n",
    "    Wrapper around getter method provided with the dataset.\n",
    "    \"\"\"\n",
    "    h5 = getters.open_h5_file_read(filename)\n",
    "    track_id = getters.get_num_songs(h5)\n",
    "    h5.close()\n",
    "    return track_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_track_id(filename):\n",
    "    \"\"\"\n",
    "    Wrapper around getter method provided with the dataset.\n",
    "    \"\"\"\n",
    "    h5 = getters.open_h5_file_read(filename)\n",
    "    track_id = getters.get_track_id(h5)\n",
    "    h5.close()\n",
    "    return track_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(filename):\n",
    "    \"\"\"\n",
    "    Wrapper around getter method provided with the dataset.\n",
    "    \"\"\"\n",
    "    h5 = getters.open_h5_file_read(filename)\n",
    "    title = getters.get_title(h5).decode()\n",
    "    h5.close()\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_artist_name(filename):\n",
    "    \"\"\"\n",
    "    Wrapper around getter method provided with the dataset.\n",
    "    \"\"\"\n",
    "    h5 = getters.open_h5_file_read(filename)\n",
    "    artist_name = getters.get_artist_name(h5).decode()\n",
    "    h5.close()\n",
    "    return artist_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year(filename):\n",
    "    \"\"\"\n",
    "    Wrapper around getter method provided with the dataset.\n",
    "    \"\"\"\n",
    "    h5 = getters.open_h5_file_read(filename)\n",
    "    year = getters.get_year(h5)\n",
    "    h5.close()\n",
    "    return year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an additional dataset obtained from the million song website that contains genre information, create genre_dataset. This dataset contains genre information for 191401 songs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Create genre dataframe\\ngenre_dataset = pd.read_table(os.path.join(data_dir, \\'tagtraum_genre_annotations/genre_dataset.txt\\'),\\n                              delimiter =\\'\\t\\', names=(\\'track_id\\', \\'genre\\'), index_col = \\'track_id\\')\\n\\n\\n# remove first row \\ngenre_dataset = genre_dataset.iloc[1:]\\n# remove comma\\ngenre_dataset[\\'genre\\'] = genre_dataset[\\'genre\\'].str[:-1]\\n\\nprint(genre_dataset.shape)\\ngenre_dataset.head()\\n\\n# Another helpful dataset obtained from the website of million song dataset is one that contains year the track was released. In addition to year, it also contains track_id, the name of the artist, and title of the track. This dataset contains 515576 data points. \\n\\n# Create year dataframe \\nyear_dataset = pd.read_table(os.path.join(data_dir, \\'tracks_per_year.txt\\'),delimiter =\\'<SEP>\\',\\n                             names=(\\'year\\',\\'track_id\\', \\'artist_name\\',\\'title\\'), index_col = \\'track_id\\')\\n\\nprint(year_dataset.shape)\\nyear_dataset.head()\\n\\nyear_artist_name_title_genre = pd.merge(year_dataset, genre_dataset, left_index=True, right_index=True)\\nyear_artist_name_title_genre[[\\'artist_name\\',\\'title\\']] = year_artist_name_title_genre[[\\'artist_name\\',\\'title\\']].apply(lambda x: x.str.lower())\\n\\nprint(year_artist_name_title_genre.shape)\\nyear_artist_name_title_genre.head()\\n\\nyear_artist_name_title_genre.year.unique()\\n\\nyear_artist_name_title_genre.genre.unique()\\n\\n# End of Genre/ Year Intersection. Number of Data points available: 152793\\n\\n# The next two cells takes about 5min to run. It takes all the .h5 files from million songs dataset and extracts the track_id, title and artist_name. It then puts all of it into a dataframe. \\n\\nMerge with \\nlo_track_id = []\\nlo_title = []\\nlo_artist_name = []\\nfor file_name in all_files:\\n    track_id = get_track_id(file_name)\\n    lo_track_id.append(track_id.decode(\"utf-8\"))\\n    title = get_title(file_name)\\n    lo_title.append(title.lower())\\n    artist_name = get_artist_name(file_name)\\n    lo_artist_name.append(artist_name.lower())\\n\\nmillion_song = pd.DataFrame(\\n    {\\'track_id\\': lo_track_id,\\n     \\'artist_name\\': lo_artist_name,\\n     \\'title\\': lo_title\\n    })\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Create genre dataframe\n",
    "genre_dataset = pd.read_table(os.path.join(data_dir, 'tagtraum_genre_annotations/genre_dataset.txt'),\n",
    "                              delimiter ='\\t', names=('track_id', 'genre'), index_col = 'track_id')\n",
    "\n",
    "\n",
    "# remove first row \n",
    "genre_dataset = genre_dataset.iloc[1:]\n",
    "# remove comma\n",
    "genre_dataset['genre'] = genre_dataset['genre'].str[:-1]\n",
    "\n",
    "print(genre_dataset.shape)\n",
    "genre_dataset.head()\n",
    "\n",
    "# Another helpful dataset obtained from the website of million song dataset is one that contains year the track was released. In addition to year, it also contains track_id, the name of the artist, and title of the track. This dataset contains 515576 data points. \n",
    "\n",
    "# Create year dataframe \n",
    "year_dataset = pd.read_table(os.path.join(data_dir, 'tracks_per_year.txt'),delimiter ='<SEP>',\n",
    "                             names=('year','track_id', 'artist_name','title'), index_col = 'track_id')\n",
    "\n",
    "print(year_dataset.shape)\n",
    "year_dataset.head()\n",
    "\n",
    "year_artist_name_title_genre = pd.merge(year_dataset, genre_dataset, left_index=True, right_index=True)\n",
    "year_artist_name_title_genre[['artist_name','title']] = year_artist_name_title_genre[['artist_name','title']].apply(lambda x: x.str.lower())\n",
    "\n",
    "print(year_artist_name_title_genre.shape)\n",
    "year_artist_name_title_genre.head()\n",
    "\n",
    "year_artist_name_title_genre.year.unique()\n",
    "\n",
    "year_artist_name_title_genre.genre.unique()\n",
    "\n",
    "# End of Genre/ Year Intersection. Number of Data points available: 152793\n",
    "\n",
    "# The next two cells takes about 5min to run. It takes all the .h5 files from million songs dataset and extracts the track_id, title and artist_name. It then puts all of it into a dataframe. \n",
    "\n",
    "Merge with \n",
    "lo_track_id = []\n",
    "lo_title = []\n",
    "lo_artist_name = []\n",
    "for file_name in all_files:\n",
    "    track_id = get_track_id(file_name)\n",
    "    lo_track_id.append(track_id.decode(\"utf-8\"))\n",
    "    title = get_title(file_name)\n",
    "    lo_title.append(title.lower())\n",
    "    artist_name = get_artist_name(file_name)\n",
    "    lo_artist_name.append(artist_name.lower())\n",
    "\n",
    "million_song = pd.DataFrame(\n",
    "    {'track_id': lo_track_id,\n",
    "     'artist_name': lo_artist_name,\n",
    "     'title': lo_title\n",
    "    })\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>track_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TRSGHLU128F421DF83</th>\n",
       "      <td>1922</td>\n",
       "      <td>alberta hunter</td>\n",
       "      <td>don't pan me</td>\n",
       "      <td>Blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRRAHXQ128F42511FF</th>\n",
       "      <td>1922</td>\n",
       "      <td>barrington levy</td>\n",
       "      <td>looking my love</td>\n",
       "      <td>Reggae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRFAFTK12903CC77B8</th>\n",
       "      <td>1922</td>\n",
       "      <td>barrington levy</td>\n",
       "      <td>warm and sunny day</td>\n",
       "      <td>Reggae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRTRKSF12903CFEDD7</th>\n",
       "      <td>1924</td>\n",
       "      <td>vernon dalhart</td>\n",
       "      <td>wreck of the old 97</td>\n",
       "      <td>Country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRQYSYF128F935F350</th>\n",
       "      <td>1925</td>\n",
       "      <td>bessie smith</td>\n",
       "      <td>careless love blues</td>\n",
       "      <td>Blues</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    year      artist_name                title    genre\n",
       "track_id                                                               \n",
       "TRSGHLU128F421DF83  1922   alberta hunter         don't pan me    Blues\n",
       "TRRAHXQ128F42511FF  1922  barrington levy      looking my love   Reggae\n",
       "TRFAFTK12903CC77B8  1922  barrington levy   warm and sunny day   Reggae\n",
       "TRTRKSF12903CFEDD7  1924   vernon dalhart  wreck of the old 97  Country\n",
       "TRQYSYF128F935F350  1925     bessie smith  careless love blues    Blues"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving to csv\n",
    "\n",
    "# year_artist_name_title_genre.to_csv(os.path.join(data_dir + \"/year_artist_name_title_genre.csv\"), encoding='utf-8')\n",
    "\n",
    "# Loading\n",
    "\n",
    "year_artist_name_title_genre = pd.read_csv(os.path.join(data_dir + \"/year_artist_name_title_genre.csv\")).set_index('track_id')\n",
    "year_artist_name_title_genre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rock          62344\n",
       "Electronic    18043\n",
       "Pop           11058\n",
       "Jazz          10662\n",
       "Rap            8156\n",
       "Metal          8035\n",
       "RnB            7692\n",
       "Country        6652\n",
       "Reggae         5475\n",
       "Blues          4228\n",
       "Folk           3484\n",
       "Punk           2739\n",
       "Latin          1954\n",
       "World          1384\n",
       "New Age         887\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_artist_name_title_genre['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11e0e7080>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Value count by Genre \n",
    "value_count_by_genre = year_artist_name_title_genre['genre'].value_counts().plot(kind = 'bar')\n",
    "value_count_by_genre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000    87300\n",
      "1990    36693\n",
      "1980    13201\n",
      "1970     8340\n",
      "1960     3580\n",
      "2010     2399\n",
      "1950      944\n",
      "1940      138\n",
      "1930      111\n",
      "1920       87\n",
      "Name: decade, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x120ab8d68>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# value \n",
    "year_artist_name_title_genre['decade'] = (year_artist_name_title_genre['year'] // 10) * 10\n",
    "#year_artist_name_title_genre = year_artist_name_title_genre.sort_index(axis = 1)\n",
    "value_count_by_year = year_artist_name_title_genre['decade'].value_counts().plot(kind = 'bar')\n",
    "\n",
    "print(year_artist_name_title_genre['decade'].value_counts())\n",
    "value_count_by_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arthur's code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import lyricfetcher\n",
    "\n",
    "def scrape_lyrics(artists_list,songs_list):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    lyrics_not_found = []\n",
    "    if (len(artists_list) == 0 or len(songs_list) == 0):\n",
    "        raise ListError('The provided artists list or songs list is empty')\n",
    "        \n",
    "    elif (len(artists_list) != len(songs_list)):\n",
    "        raise ListError('The provided artists and songs lists have different lenghts')\n",
    "        \n",
    "    else:\n",
    "        lyrics = []\n",
    "        \n",
    "        for i in range(len(artists_list)):\n",
    "            try:\n",
    "                lyrics_metro = str(lyricfetcher.get_lyrics('metrolyrics',artists_list[i],songs_list[i]))\n",
    "                lyrics_metro = re.sub(r'[\\[].*?[\\]]', '', lyrics_metro.replace('\\n', ' '))\n",
    "                lyrics_metro = re.sub(',', '', lyrics_metro)\n",
    "\n",
    "                if (lyrics_metro == \"\" or len(lyrics_metro.split())<3):\n",
    "                    lyrics_az = str(lyricfetcher.get_lyrics('azlyrics',artists_list[i],songs_list[i]))\n",
    "                    lyrics_az = re.sub(r'[\\[].*?[\\]]', '', lyrics_az.replace('\\n', ' '))\n",
    "                    lyrics_az = re.sub(',', '', lyrics_az)\n",
    "\n",
    "                    if (lyrics_az == \"\" or len(lyrics_az.split())<3):\n",
    "                        lyrics.append(\"\")\n",
    "                        raise ValueError('No lyrics OR lyrics smaller or equal to 2 words found')\n",
    "\n",
    "                    else:\n",
    "                        lyrics.append(lyrics_az)\n",
    "\n",
    "                else:\n",
    "                    lyrics.append(lyrics_metro)\n",
    "                    \n",
    "            except ValueError:\n",
    "                lyrics_not_found.append([artists_list[i],songs_list[i]])\n",
    "                continue\n",
    "    return lyrics, lyrics_not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', \"What is she covering up? What is she lying about? She's hesitating... And she's using slow words I know But she's moving fast. Why is this happening now Why does this happen to me - all the time? Don't give up on me (Love) And I won't give up on you. (Love) Squeeze me 'til I can't breathe. And lets just lie here In oblivion. I can't believe in one of us I can't know - how long This love will last. Stay up in bed with me Stay up and play with me All day long. Don't give up on me (Love) And I won't give up on you. (Love) I get to wait for the baby (Love me girl) Nothing is wrong what are you scared of? Squeeze me 'til I can't breathe. And lets just lie here In oblivion. I can't believe in one of us I can't know - how long This love will last. Stay up in bed with me Stay up and play with me All day long. Don't give up on me (Love) And I won't give up on you (Love) I get to wait for the baby (Love me girl) Love me girl.\", \"Too much too much is never enough I had you and I gave you up No idea where my mind was for months I woke up I cashed in all of my luck Walked hand and hand with your trust And everybody was kissing on fire And we all got burnt It'd be safer to hate her Than to love her and to lose her It'd be safer to hate her all around Caught you having a laugh Did you catch me have the last? I've been smiling like this for days Just to make up for my mistakes In the dark I watch everyone disappear And I am beginning to let myself down I am pushing everyone that was in out It'd be safer to hate her Than to love her and to lose her It'd be safer to hate her all around And nobody knows what it's like To live and die on the inside Nobody knows what it's like To be one of a kind When we die do we feel alive? When we die do we feel alive? It'd be safer to hate her Than to love her and to lose her It'd be safer to hate her all around And nobody knows what it's like To live and die on the inside And nobody knows\", \"So this is the end of you and me We had a good run and I'm setting you free To do as you want to do as you please Without me Remember when you were my boat and I was your sea? Together we'd float so delicately But that was back when we could talk about anything 'Cause I don't know who I am When you're running circles in my head And I don't know just who you are When you're sleeping in someone else's bed Three whole words and eight letters late And that would have worked on me yesterday We're not the same I wish that could change But it can't And I'll say your name and in the same breath I'll say something that I'll grow to regret So keep your hands on your chest and sing with me That we don't wanna believe 'Cause I don't know who I am When you're running circles in my head And I don't know just who you are When you're sleeping in someone else's bed So it's true what they say If you love someone you should set them free Oh it's true what they say When you throw it away I don't know who you are I don't know who you are Oh 'cause I don't know who you are When you sleep with somebody else 'Cause I don't know who I am When you're sleeping with him It's true what they say when you throw it away\", \"If one drink can make tonight slip my mind then I Should drink up so I can forget that I have lived my life You are an example of better things to come So I'll wait on some other escape that leads me nowhere fast I've gotta ask You've got nothing to lose Except for me and you And I don't want that attitude When you know I can do I'll do better than you If one can drink can make tonight slip your mind then you Should drink up so you can convince yourself that I'm cute We are an example of why not to fall in love It takes a turn and then it hurts More than you could dream of When you've got nothing to lose Except for me and you And I don't want that attitude When you know I can do I'll do better than you And you've got nothing to prove Oh no wait yes you do You wear it so well that we think it's true You can't stay I want you gone You're pulling the carpet I was standing on You can't stay I want you gone You're pulling the carpet I was standing on You can't stay I want you gone You're pulling the carpet I was standing on Here's the exits exits exits You've got nothing to lose Except for me and you And I don't want that attitude When you know I can do I'll do better than you We are an example of (Why not to fall in why not to fall in love) We are an example of (Why not to fall in why not to fall in love)\", \"It's worse worse than you think If truth be told I'll tell you that you're lovesick I am sweet so everybody takes a bite Dig in I'm the catalyst of your demise. Words are cheap so I'll sell you a line Like the way you acted that night Second best is what what you want then It won't cost that much but me. I've got you hanging on every word I say But that don't mean a thing And you love the way I take your breath away So I'll take your breath away. Have you had had your day? Move close and I'll push you away. Play it safe For the sake of keeping some face Play it safe In the name of keeping your place. Do what what I tell you to With arms we'll carry this through Can't wait can't wait to see you. I've got you hanging on every word I say But that don't mean a thing And you love the way I take your breath away So I'll take take take. Oh I only have eyes for you And it's so true. Oh I only have eyes for you It's the truth it's the truth. So true. So true. So true. So true. And I only have eyes for you And I only have eyes for you. I've got you hanging on every word I say But that don't mean a thing And you love the way I take your breath away So I'll take take take. Oh I only have eyes for you And it's so true. Oh I only have eyes for you It's the truth it's the truth. So true.\", \"If I could have a minute please Then I'd bring you to your knees This contagious chemistry is killing me Oh you'll never disappear Until I give permission dear 'Cause you're always gonna always be needy to please Dearest enemy I fear that You will just rush me along the way Somewhere between the fake smiles and your free drinks Please don't smother me I swear that I need some room to breathe while with you All up down and over me You're not a name you're just a face It's contagious so catch it This love is dirtier than you think Don't believe your eyes Believe your ears trust me This'll sink lower than you think Don't believe your eyes Believe your ears trust me Dearest Enemy you should listen to the streets Because they tell you all you need to know 'Bout who and what you are No smoke without a fire With that I'll name drop you a liar It suits your skin and bones You've known all along It's exactly what you are It's contagious so catch it This love is dirtier than you think Don't believe your eyes Believe your ears trust me This'll sink lower than you think Don't believe your eyes Believe your ears trust me This is the first and the last time I swear There's more where that came from She said do you know what that means? 'Cause I know what that means This is the first and the last time I swear It's a numbers game and you were there Do you know what this means? 'Cause we know what this means No (Oh!) Who'd you think I was? Yeah And oh (Oh!) This leads to the only lead you were working on And oh (Oh!) And you were hopeless at best yeah And oh forget the rest Hotel rooms cheap thrill dress Lack of common sense Could make this happen If I could have a minute please Then I'd being you to your knees This contagious contagious chemistry Dearest Enemy You should have never trusted me You bitch\", \"You're not on my list of things to do 'cause I've already done you. And I am protective it's so cute but only when it suits you. Is this all in vain? Can these words explain Desperate minds mean desperate measures You've got to get this one together You're young and in love That should be enough. At least you'll have nice thoughts of me when I am cheating on you In your dreams. I've told you before my closet's clean and that these bones don't belong to me. Try this one for size try me one more time. You hold me down by keeping me around. Desperate minds mean desperate measures You've got to get this one together You're young and in love That should be enough. Desperate minds mean desperate measures You've got to get this one together You're young and in love That should be enough. You hold me down by keeping me around. And no one takes me home. no one takes me home. You hold me down by keeping me around. And no one takes me home. no one takes me home. Desperate minds mean desperate measures You've got to get this one together You're young and in love That should be enough. (Young and in love that should be enough.)\", \"You've got a lot to say For the one that walked away I give you take It's the way it's always been Oh how do I know If I should stay or just go? The bottom line is this way That I'll never know Stay with me Stay with me stay with me Stay with me oh Whoa whoa whoa Whoa whoa whoa You've got a lot to say For the one that pushed me away I give you take Some things they never change Just change Stay with me Stay with me stay with me Stay with me I never knew that I could be this way I never knew that I could walk away These things take time to grow It's been said that time heals wounds But no I won't be controlled And so the story goes Stay with me Stay with me stay with me Stay with me And now I know that I can be this way And now I know that I can walk away\", \"Is this hard to swallow You keep your thoughts by your pillow Just so you can adore them Just so we can ignore them I'd rather lead not follow I'd rather stay and not go home My moves have got so old or so I've been told But now you've lost it all We're running out of time We've got things on our mind And things we just don't like Who do you want to see? Who do you want to be tonight? Forgetting the promises you make Is how your your promises easily break Oh you sit on your fence And I'll dig under it I'll count my losses And I'll count my sins I hope you've cleared your mind We're running out of time We've got things on our mind And things we just don't like Who do you want to see? Who do you want to be tonight? We're running out of time We've got things on our mind And things we just don't like Who do you want to see? Who do you want to be tonight? We were young and we were blind We ignored the warning signs How were we to know? How were we to know? We were close but still so far We grew apart and out of touch All I wanted was... All I wanted was to say... We're running out of time We've got things on our mind And things we just don't like Who do you want to see? Who do you want to be tonight? We're running out of time We've got things on our mind And things we just don't like Who do you want to see? Who do you want to be tonight? Tonight tonight tonight tonight\", \"Calm down she said Consider this a warning A souvenir for the morning A headache that you can't fix I said I can talk my my way out of anything But I am struggling in this emergency This one's on your side I said this one's on your Ambulance I'm calling you now Accidents bring the house down Come on come on you don't know me Come on come on you owe me nothing Dry your eyes and stick them on ice Give your chest a rest it's been cold your whole life I'll have you know the tables are about to turn And you're going to get what what what you deserve Ambulance I'm calling you now Accidents bring the house down Come on come on you don't know me Come on come on you owe me nothing Is this in it my love? Turn it up turn it up I want to hear you scream Oh is this in it my love? Turn it up turn it up I want to hear you scream So sing I am my own worst enemy That's what she said to me And I am living out your dream So sing I am my own worst enemy So we can just breathe a little more safely Ambulance I'm calling you now Accidents bring the house down Come on come on you don't know me Come on come on you owe me nothing Is this in it my love? Turn it up turn it up I want to hear you scream Oh is this in it my love? Turn it up turn it up I want to hear you scream sing I've got real big plans and such bad thoughts I've got real big plans and such bad thoughts I've got real big plans and such bad thoughts I've got real big plans Real big plans and such bad thoughts I've got real big plans and such bad thoughts I've got real big plans and such bad thoughts I've got real big plans I've got real big plans and such bad thoughts I've got real big plans and such bad thoughts I've got real big plans and such bad thoughts I've got real big plans and such bad thoughts I've got real big plans and such bad thoughts I've got real big plans and such bad thoughts I've got real big plans and such bad thoughts I've got real big plans and such bad thoughts I've got real big plans and such bad thoughts I've got real big plans and such bad thoughts I've got real big plans and such bad thoughts I've got real big plans and such bad thoughts\", '', '', \"I'm tired of playing on the team It seems I don't get time out anymore What a change if we set the pace face to face No one even trying to score Oh oh I can feel the magic of your touch And when you move in close a little bit means so much Ooh yeah you've got to understand baby Time out is what I'm here for One on one I wanna play that game tonight One on one I know I wanna play that One on one I wanna play that game tonight One on one so slow You can't tell me you don't miss me girl I think I might know you too well Wonder what you'd say if you knew that I was coming tonight Want to? I want you can't you tell One on one I wanna play that game tonight One on one I know I wanna play that One on one I wanna play that game tonight One on one so slow That's all you need to know now 'Cause if it's really right there's nothing else One on one I want to play that game tonight...\", \"Everybody's high on consolation Everybody's trying to tell me what is right for me yeah My daddy tried to bore me with a sermon But it's plain to see that they can't comfort me Sorry Charlie for the imposition I think I got (it got it) I got the strength to carry on yeah I need a drink and a quick decision Now it's up to me ooh what will be She's gone she's gone Oh I oh I I better learn how to face it She's gone she's gone Oh I oh I I'd pay the devil to replace her She's gone and she's gone Oh why what went wrong? Get up in the morning look in the mirror I'm worn as a toothbrush hanging in the stand yeah My face ain't looking any younger Now I can see love's taken a toll on me She's gone she's gone Oh I oh I I better learn how to face it She's gone and she's gone Oh I oh I I'd pay the devil to replace her She's gone she's gone Oh why what went wrong? Think I'll spend eternity in the city Let the carbon and monoxide choke my thoughts away yeah And pretty bodies help dissolve the memories They can never be what she was (was) to me She's gone she's gone Oh I oh I I better learn how to face it She's gone she's gone Oh I oh I I'd pay the devil to replace her She's gone and she's gone Oh I what went wrong? She's gone Oh I I better learn how to face that She's gone she's gone I can't believe that she's gone Oh I I'd pay the devil to replace her She's gone Oh I I I better learn how to face that She's gone she's gone I can't believe that she's gone Oh I I pay the devil to replace her She's gone she's gone She's gone she's gone She's gone she's gone She's gone she's gone She's gone she's gone She's gone she's gone She's gone she's gone She's gone she's gone\", \"When we first met it wasn't what you said And still I loved you like mad I loved you like mad When we first met they were playing that song And then it stuck into my head stuck into my head When we first kissed it made it to my list And I couldn't stop myself think of nothing else When we parked the car they were playing that song And we turned it up to ten and started up again Every time I hear it play I think of you and those summer days Oh I can still remember When I heard it on the radio Oh but now we are September Come on forget what we know When you broke my heart there was nothing you could say And still I loved you like mad I loved you like mad Now that I think back it wasn't what we had They were just playing that song made it last so long Every time I hear it play I think of you and those summer days Oh I can still remember When I heard it on the radio Oh but now we are September Come on forget what we know Oh I can still remember When I heard it on the radio Oh but now we are September Come on forget what we know Oh I can still remember When I heard it on the radio Oh but now we are September Come on forget what we know\"]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[['xenia beliayeva', 'analog effekt'], ['year long disaster', 'stranger in my room'], ['year long disaster', 'she told us all'], ['year long disaster', 'cyclone'], ['zpu', 'revolucin'], ['the bird and the bee', 'maneater'], ['the bird and the bee', 'private eyes']]\n",
      "Count of not found lyrics : 7\n"
     ]
    }
   ],
   "source": [
    "# Test on subset\n",
    "\n",
    "lyrics, lyrics_not_found_list = scrape_lyrics(year_artist_name_title_genre['artist_name'].tail(20),year_artist_name_title_genre['title'].tail(20))\n",
    "\n",
    "print(lyrics)\n",
    "print(lyrics_not_found_list)\n",
    "print('Count of not found lyrics : ' + str(len(lyrics_not_found_list)))\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(os.path.join(data_dir + \"/lyrics.csv\"), \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows([year_artist_name_title_genre.index.values.tolist(),lyrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function links gets the genres for a single track given its id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_song_genres(track_id):\n",
    "    \"\"\"\n",
    "    Get's the genres of a song given it's ID in a single string, separated by &.\n",
    "    \"\"\"\n",
    "    if track_id in genre_dataset.index:\n",
    "        return \"&\".join(genre_dataset.loc[[track_id]].values[0][0].split(' and '))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting our data collection, we make sure that all files correspond to only one song as they should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in tqdm(all_files): # tqdm for a nice progress bar\n",
    "    assert get_num_songs(filename) == 1 # check whether each file correctly corresponds to a single song."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet reads the Million Song dataset in its entirety and uses the genre dataset to link the two. It gets all the fields we need as we discussed above and also gets the genres of each track. This information is then put into a dataframe. \n",
    "\n",
    "For convenience, we save this data in a new `.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataframe is saved into a file for convenience.\n",
    "\"\"\"\n",
    "data = pd.DataFrame([])\n",
    "\n",
    "for filename in tqdm(all_files):\n",
    "    track_id = get_track_id(filename).decode()\n",
    "    genres = get_song_genres(track_id)\n",
    "    if genres:\n",
    "        to_add = [('track_id', track_id), ('genres', genres), ('artist_name', get_artist_name(filename)), ('title', get_title(filename)), ('year', get_year(filename)), ('lyrics', \"\")]\n",
    "        data = data.append(pd.DataFrame(OrderedDict(to_add), index=[0]))\n",
    "\n",
    "data.set_index('track_id', inplace=True)\n",
    "data.to_csv('data/data.csv')\n",
    "\"\"\"\n",
    "\n",
    "data = pd.read_csv('data/data.csv').set_index('track_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to obtain lyrics data for our tracks. For this, we have found two datasets. Both of these contain artist, track title and lyrics data which we read in the following code snippets. We try to get the lyrics from both datasets, but it's possible that neither of them contains the lyrics for some on our tracks. For this reason, we will also look at genius.com which is a website containing many lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_df1 = pd.read_csv('lyrics/songdata1.csv')\n",
    "lyrics_df1.set_index(['artist', 'song'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_df2_raw = pd.read_csv('lyrics/songdata2.csv', na_filter=False)\n",
    "lyrics_df2 = lyrics_df2_raw[['song', 'artist', 'lyrics']].set_index(['artist', 'song'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lyrics_csv1(artist_name, title):\n",
    "    \"\"\"\n",
    "    Gets lyrics for an artist and title pair from the first dataset.\n",
    "    \"\"\"\n",
    "    if (artist_name, title) in lyrics_df1.index:\n",
    "        return lyrics_df1.loc[artist_name, title].values[0][1]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lyrics_csv2(artist_name, title):\n",
    "    \"\"\"\n",
    "    Gets lyrics for an artist and title pair from the second dataset.\n",
    "    In this file, the artist and title fields have hyphens instead of spaces\n",
    "    and are exclusively in lower case, so we change our data to match this\n",
    "    format when looking for songs.\n",
    "    \"\"\"\n",
    "    index_artist_name = artist_name.lower().replace(' ', '-')\n",
    "    index_title = title.lower().replace(' ', '-')\n",
    "    if (index_artist_name, index_title) in lyrics_df2.index:\n",
    "        lyrics = lyrics_df2.loc[index_artist_name, index_title].values[0][0]\n",
    "        if len(lyrics) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return lyrics\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lyrics(artist_name, title):\n",
    "    \"\"\"\n",
    "    Gets the lyrics for an artist name and title from both datasets.\n",
    "    If lyrics are not found in either of them, returns an empty string.\n",
    "    \"\"\"\n",
    "    lyrics = get_lyrics_csv1(artist_name, title)\n",
    "    if lyrics:\n",
    "        return lyrics\n",
    "    \n",
    "    lyrics = get_lyrics_csv2(artist_name, title)\n",
    "    if lyrics:\n",
    "        return lyrics\n",
    "    \n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a new dataframe that contains lyrics information for our previous data. If the lyrics are not found in either of the lyrics datasets, we generate the genius.com url to search for that song's lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Match lyrics\n",
    "data_lyrics = data.copy()\n",
    "urls = {}\n",
    "i = 1\n",
    "for index, row in data.iterrows():\n",
    "    lyrics = get_lyrics(row['artist_name'], row['title'])\n",
    "    \n",
    "    # We generate an URL to lookup in Genius if we haven't found any lyrics\n",
    "    # in the first two datasets.\n",
    "    if lyrics == \"\":\n",
    "        # To create the URL to find the song on genius, the title and artist names\n",
    "        # need to be processed to match the general format of Genius' songs URL.\n",
    "        # For instance, spaces are replaced by hyphens and additional information\n",
    "        # between parenthesis is removed.\n",
    "        url = (row['artist_name'].lower().replace(' ', '-') + '-' + re.sub(r'\\([^)]*\\)', '', row['title']).rstrip().lower().replace(' ', '-') + '-lyrics').capitalize().replace(\"'\", '')\n",
    "        urls[index] = 'https://genius.com/' + url\n",
    "        \n",
    "    print(i, end='\\r')\n",
    "    i += 1\n",
    "    data_lyrics.loc[index, 'lyrics'] = re.sub(r'[\\[].*?[\\]]', '', lyrics.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The genius.com URLs are collected in a file so that they can be fed into a scrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/urls', 'w') as urls_files:\n",
    "    for index, url in urls.items():\n",
    "        print(index, url, file=urls_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we run our scrapper which is using `scrapy`. This is not done in this notebook but instead you can find the scrapper code in the `scrapper` folder in this repository. We obtain a file that contains the track ids as well as their lyrics found on genius.com.\n",
    "\n",
    "The resulting file is then read and its data is added to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data/missing_lyrics.json') as lyrics_file:\n",
    "    lyrics_json = json.load(lyrics_file)\n",
    "    for item in lyrics_json:\n",
    "        for index, lyrics in item.items():\n",
    "            # Returned lyrics are modified to remove newline cahracters as well as some\n",
    "            # special lyrics structures as we have seen before.\n",
    "            data_lyrics.loc[index, 'lyrics'] = re.sub(r'[\\[].*?[\\]]', '', lyrics.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then detect the language of the lyrics if any. It is possible that some of the lyrics do not contain any features that allow language detection, in this case we do not assign a language to that lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the corresponding language for the lyrics\n",
    "from langdetect import detect\n",
    "\n",
    "for index, row in data_lyrics.iterrows():\n",
    "    lyrics = data_lyrics.loc[index, 'lyrics']\n",
    "    language = None\n",
    "    if lyrics.strip() != \"\":\n",
    "        try:\n",
    "            language = detect(lyrics)\n",
    "        except:\n",
    "            language = \"\"\n",
    "    if language != \"\":\n",
    "        data_lyrics.loc[index, 'lang'] = language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how many songs are there with lyrics in the Million Song Subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lyrics = data_lyrics[data_lyrics.lyrics != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lyrics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the resulting data in a file for convenience. This is the final state of our data and contains verything we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lyrics.to_csv('data/data_lyrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain our analysis, we will follow the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lyrics Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to be able to extract different themes from our lyrics. Other than being able to see the evolution of some words over time and depending of the genres of the song, it's interesting to see the themes or sentiments that the song's lyrics portray. For this, we will use Natural Language Processing (NLP) libraries to extract this information about each track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the lyrics that we have, we apply the bag-of-words model and only keep the interesting (meaningful) words. That is, we remove the stop words and lemmatize each word to avoid, for instance, having both 'sleep' and 'sleeping'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by downloading `nltk` packages that will be of use for us. [ntlk](http://www.nltk.org/) is a well known framework for natural language processing in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define a few functions to do all our natural language analysis steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a function that tokenizes lyrics. As we have seen in class, working on a list of tokens instead of a string of characters is much better for machine learning and natural language processing techniques that we will use. For this we use TextBlob which we will also use later on for sentiment analysis since it gives a nice way to obtain tagged tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return TextBlob(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will define a function that removes stopwords from our tokens. This function uses standard stop words list from `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_language_full_name(isocode):\n",
    "    return pycountry.languages.get(alpha_2=isocode).name.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_languages = {}\n",
    "# Other than the stopwords for a given language which we obtain from\n",
    "# nltk, we also add a list of common first names which we remove from\n",
    "# the lyrics to ease topic detection later on. This list was found in\n",
    "# https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/.\n",
    "names_pd = pd.read_csv('data/common_names.csv')\n",
    "\n",
    "names = []\n",
    "for name in names_pd.values:\n",
    "    names.append(name[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if we only have English lyrics, the code below can support the stopwords from multiple languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(blob, language):\n",
    "    if language not in stop_words_languages:\n",
    "        stop_words_languages[language] = set(stop_words.words(get_language_full_name(language)))\n",
    "        if language == 'en':\n",
    "            # Additional stopwords not caught by nltk's list.\n",
    "            stop_words_languages[language] |= set(['na', 'gon', 'la', 'nt', 'i', '', \"'\"])\n",
    "            stop_words_languages[language] |= set(names)\n",
    "        \n",
    "    tokens = []\n",
    "    for word, tag in blob.tags:\n",
    "        lower = word.lower().replace(\"'\", '')\n",
    "        if lower not in stop_words_languages[language]:\n",
    "            tokens.append((lower, tag))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next function, we lemmatize the tokens that are left so that words that variants of words that are essentially the same (conjugated verbs for examples) are counted as the same token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(tokens):\n",
    "    lemmas = []\n",
    "    lemma = None\n",
    "    for token, tag in tokens:\n",
    "        if tag[0] == \"V\": #if the word is a verb\n",
    "            lemma = Word(token).lemmatize(\"v\")\n",
    "        else:\n",
    "            lemma = Word(token).lemmatize()\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function combines the functions defined above to get the final tokens that we will consider for our topic detection task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_tokens(lyrics):\n",
    "    \"\"\"\n",
    "    Combines our functions to get the final list of tokens.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for lyric in lyrics:\n",
    "        texts.append(lemmatize(remove_stopwords(tokenize(lyric), 'en')))\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our data visualisation, we also need to have raw frequencies of the words that we use. While later on we will use different ways to obtain such information for our ML techniques, here for the visualisation we want a quick and simple count of the appearence of our tokens over the whole corpus passed as parameters. This way we can easily obtain the frequency of words appearing in all the lyrics for a given genre in a given year for example. The function defined below does precisely that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_freq(texts):\n",
    "    \"\"\"\n",
    "    Gets the raw word count of each word appearing in\n",
    "    the lyrics given by texts.\n",
    "    \"\"\"\n",
    "    word_count = {}\n",
    "    for text in texts:\n",
    "        for token in text:\n",
    "            if token not in word_count:\n",
    "                word_count[token] = 1\n",
    "            else:\n",
    "                word_count[token] += 1\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the general sentiment of a song's lyrics, we use [TextBlob](https://textblob.readthedocs.io/en/dev/). This library has already a built-in sentiment analyser, which gives us inforation whether the song is 'positive' or 'negative'. The following function is used to do just that and takes as parameter a single lyrics string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(lyrics):\n",
    "    \"\"\"\n",
    "    Gets the sentiment polarity of a lyrics given as string.\n",
    "    \"\"\"\n",
    "    blob = TextBlob(lyrics)\n",
    "    return blob.sentiment.polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have everything needed defined, we begin by getting our data (from the file written to in the previous section) into a dataframe. We only keep the lyrics that we detected to be in english since all our NLP and ML techniques wouldn't work on lyrics from different languages.\n",
    "\n",
    "We construct a genres list that will be used later in our data visualisation as well as a dictionary that links the genres to a list of indices of particular songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_df = pd.read_csv('data/data_lyrics.csv')\n",
    "lyrics_df.set_index(['track_id'], inplace=True)\n",
    "\n",
    "lyrics_df = lyrics_df[lyrics_df.lang == 'en']\n",
    "genres_list = ['all'] # Added a special genre, 'all' which will capture all songs\n",
    "genres_indices = {}\n",
    "for index, row in lyrics_df.iterrows():\n",
    "    genre = row['genres']\n",
    "    genre = genre.replace('&', ',') # Replaced for later on when we use it in HTML\n",
    "    genres_indices.setdefault(genre, []).append(index)\n",
    "    genres_indices.setdefault('all', []).append(index)\n",
    "    if genre not in genres_list:\n",
    "        genres_list.append(genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call our functions on the lyrics of each song and insert these tokens as a new column in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_df['tokens'] = get_final_tokens(lyrics_df.lyrics.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add yet another column that is the sentiment of the lyrics of a particular song, obtained using the method described above. We scale the sentiment between 0 and 1 for our final visualisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lyrics = lyrics_df.lyrics.values\n",
    "sentiments = []\n",
    "for lyric in all_lyrics:\n",
    "    sentiment = get_sentiment(lyric)\n",
    "    sentiment = (sentiment + 1) / 2.0\n",
    "    sentiments.append(sentiment)\n",
    "lyrics_df['sentiment'] = sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we collect all our remaining data and output it into files that we will use for our visualisation. We first begin by aggregating the word frequencies by genre and year of release (songs without a year of release are dropped). Each word results in a file where each datapoint corresponds to a year and the frequency of the word in the particular genre and the particular year.\n",
    "\n",
    "The sentiment files are done in the same way, except that there is one file per genre.\n",
    "\n",
    "Moreover, a list of all words appearing for every genre is also created for our final data visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will collect here a list of words for each genre\n",
    "# for our visualisation's autocomplete feature.\n",
    "genres_words = {}\n",
    "to_save = {}\n",
    "for genre, indices in genres_indices.items():\n",
    "    curr_df = lyrics_df.loc[indices]\n",
    "    genres_words[genre] = []\n",
    "    \n",
    "    # create the directory for words in each genre\n",
    "    directory_words = 'data/final_data/words/' + genre\n",
    "    if not os.path.exists(directory_words):\n",
    "        os.makedirs(directory_words)\n",
    "    \n",
    "    # create the directory for sentiments\n",
    "    directory_sentiments = 'data/final_data/sentiments'\n",
    "    if not os.path.exists(directory_sentiments):\n",
    "        os.makedirs(directory_sentiments)\n",
    "    \n",
    "    for year in curr_df[curr_df.year != 0].year.sort_values().unique():\n",
    "        curr_year_df = curr_df[curr_df.year == year]\n",
    "        # get the frequencies of words in the current genre and year\n",
    "        freqs = get_word_freq(curr_year_df.tokens.values)\n",
    "        # get the word count over this data for proportion calculation\n",
    "        word_count = sum(freqs.values())\n",
    "        \n",
    "        # get the sentiment values and calculate the proportion\n",
    "        sentiments = curr_year_df.sentiment.values\n",
    "        sentiment_avg = sum(sentiments) / float(len(sentiments))\n",
    "        filepath = directory_sentiments + '/' + genre + '.csv'\n",
    "        if not os.path.exists(filepath):\n",
    "            with open(filepath, 'a') as output_file:\n",
    "                print('year,value', file=output_file)\n",
    "        with open(filepath, 'a') as output_file:\n",
    "            print(str(year) + ',' + str(sentiment_avg), file=output_file)\n",
    "            \n",
    "        # save word data (proportions as well)\n",
    "        for word, freq in freqs.items():\n",
    "            # remove undesirable characters from filenames\n",
    "            for ch in ['/', '*', '\"', ':', '\\\\']:\n",
    "                word = word.replace(ch, '-')\n",
    "            if word != \"\":\n",
    "                filepath = directory_words + '/' + word + '.csv'\n",
    "                # saved in a directory to later output in a file\n",
    "                # this is done so that we don't append to the file\n",
    "                # before we know that there is more than one value.\n",
    "                # having a single value in our visualisation doesn't\n",
    "                # show anything but potentially makes other graphs \n",
    "                # look bad because of scaling.\n",
    "                to_save.setdefault((filepath, word, genre), []).append(str(year) + ',' + str(freq/float(word_count)))\n",
    "\n",
    "# save word data to files\n",
    "for filepath_word, lines in to_save.items():\n",
    "    filepath, word, genre = filepath_word\n",
    "    if len(lines) > 1: # save only if more than one datapoint\n",
    "        if word not in genres_words[genre]:\n",
    "            genres_words[genre].append(word)\n",
    "        with open(filepath, 'w') as output_file:\n",
    "            print('year,value', file=output_file)\n",
    "            for line in lines:\n",
    "                print(line, file=output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finnaly, the list of genres that we created earlier will be of use for our data visualisation as an auto-complete feature when searching for genres. The same is the case for the lists of words per genre. This information is saved in files as defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for genre, words in genres_words.items():\n",
    "    with open('data/final_data/words/' + genre + '/allWords.json', 'w') as output_file:\n",
    "        json.dump(words, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/final_data/allGenres.json', 'w') as output_file:\n",
    "    json.dump(genres_list, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, to find the topics of our songs we use [Mallet](http://mallet.cs.umass.edu/topics.php).\n",
    "\n",
    "This method doesn't create a list of themes with clear names, but rather assumes that a topic is a collection of words, thus we will need to manually assign a name for each topic.\n",
    "\n",
    "The LDA model will output, for each song, a list of topics with different weights.\n",
    "\n",
    "The following code just outputs the collection of tokens with corresponding track ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'mallet/lyrics' \n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in lyrics_df.iterrows():\n",
    "    with open(path + '/' + index + '.txt', \"w\") as text_file:\n",
    "        tokens = ''\n",
    "        for token in row.tokens:\n",
    "            tokens += token + ' '\n",
    "        text_file.write(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if the number of topics is modified, you need to change it as well in the second command below\n",
    "num_topics = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process the output, just run the following command in Mallet's directory:\n",
    "\n",
    "`bin/mallet import-dir --input mallet/* --output lyrics.mallet --remove-stopwords --keep-sequence`\n",
    "\n",
    "We keep the `--remove-stopwords` just to be sure to remove all the stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then this command :\n",
    "\n",
    "\n",
    "`bin/mallet train-topics --input lyrics.mallet --num-topics 20 --num-iterations 1000 --optimize-interval 10 --output-topic-keys topics_composition.txt --output-doc-topics songs_composition.txt?`\n",
    "\n",
    "The output should be in the folder `data` for the next operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the prediction of the LDA model on the lyrics and add it to the lyrics dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_composition = pd.read_csv('data/songs_composition.txt', header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_get_id(filenames):\n",
    "    \"\"\"\n",
    "    Get the id of a song from the corresponding filename.\n",
    "    We need this because Mallet outputs the filepath and not the original ID.\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    for filename in filenames:\n",
    "        id_ = filename[1].split('/')[-1].split('.')[0]\n",
    "        ids.append(id_)\n",
    "    return ids\n",
    "\n",
    "def df_get_topics(df_topics):\n",
    "    \"\"\"\n",
    "    Get a single list of topics from the topics columns from\n",
    "    Mallets output file.\n",
    "    \"\"\"\n",
    "    list_topics = []\n",
    "    for row in df_topics:\n",
    "        topics = {}\n",
    "        for i in range(num_topics):\n",
    "            topics[i] = row[i + 2]\n",
    "        list_topics.append(topics)\n",
    "    return list_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "songs_composition['track_id'] = df_get_id(songs_composition.values)\n",
    "songs_composition['topics'] = df_get_topics(songs_composition.values)\n",
    "id_topics = songs_composition[['track_id', 'topics']]\n",
    "\n",
    "id_topics = id_topics.set_index('track_id')\n",
    "# We add the topics column to the lyrics dataset and name it result\n",
    "result = pd.concat([id_topics, lyrics_df], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the topics coefficient for each song, we can retrieve interesting information.\n",
    "The goal is to have for a given genre, the coefficient of a given topic for a given year.\n",
    "For instance to kown what is the importance of the Gangsta topics in the pop genre in the year 2007."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topics_genres = {}\n",
    "topics_all = {}\n",
    "\n",
    "# We iterate through the dataset\n",
    "for index, row in result.iterrows():\n",
    "    genre = row['genres']\n",
    "    genre = genre.replace('&', ',')\n",
    "    topics = row['topics']\n",
    "    year = row['year']\n",
    "    \n",
    "    # Add genre if not in topics_genres\n",
    "    if genre not in  topics_genres:\n",
    "        topics_genres[genre] = {}\n",
    "    \n",
    "    for topic in topics:\n",
    "        # Add topic if not in topics_genres[genres]\n",
    "        # This is where we add the topic value for a given genre\n",
    "        if not topic in topics_genres[genre]:\n",
    "            topics_genres[genre][topic] = {}\n",
    "        # Add year for this topic and initialize it\n",
    "        if not year in topics_genres[genre][topic]:\n",
    "            topics_genres[genre][topic][year] = []\n",
    "            topics_genres[genre][topic][year].append(0)\n",
    "            topics_genres[genre][topic][year].append(0)\n",
    "        # Sum the topic of this song for this year\n",
    "        topics_genres[genre][topic][year][0] += topics[topic]\n",
    "        # Increase the number of songs for this genre/topic/year \n",
    "        # in order to have a mean \n",
    "        topics_genres[genre][topic][year][1] += 1\n",
    "        \n",
    "        # Same as before but for all genre (special case)\n",
    "        if topic not in topics_all:\n",
    "            topics_all[topic] = {}\n",
    "        \n",
    "        if not year in topics_all[topic]:\n",
    "            topics_all[topic][year] = []\n",
    "            topics_all[topic][year].append(0)\n",
    "            topics_all[topic][year].append(0)\n",
    "        topics_all[topic][year][0] += topics[topic]\n",
    "        topics_all[topic][year][1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean for this genre/topic/year combination\n",
    "results = {}\n",
    "for genre in topics_genres:\n",
    "    results[genre] = {}\n",
    "    for topic in topics_genres[genre]:\n",
    "        results[genre][topic] = {}\n",
    "        for year in topics_genres[genre][topic]:\n",
    "            if year:\n",
    "                mean = topics_genres[genre][topic][year][0] / topics_genres[genre][topic][year][1] \n",
    "                results[genre][topic][year] = mean\n",
    "results\n",
    "\n",
    "# Same as above\n",
    "results_all = {}\n",
    "for topic in topics_all:\n",
    "    results_all[topic] = {}\n",
    "    for year in topics_all[topic]:\n",
    "        if year:\n",
    "            mean = topics_all[topic][year][0] / topics_all[topic][year][1] \n",
    "            results_all[topic][year] = mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final result\n",
    "path = 'data/final_data/topics'\n",
    "for genre in results:\n",
    "    for topic in results[genre]:\n",
    "        directory = path + '/' + genre + '/'\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        with open((directory + str(topic) + '.csv'), 'w') as csvfile:\n",
    "            print('year,value', file=csvfile)\n",
    "            for year in sorted(results[genre][topic].keys()):\n",
    "                csvfile.write(str(year) + ',' + str(results[genre][topic][year]) + '\\n')\n",
    "\n",
    "path = 'data/final_data/topics'\n",
    "directory = path + '/' + 'all' + '/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "for topic in results_all:\n",
    "    with open((directory + str(topic) + '.csv'), 'w') as csvfile:\n",
    "        print('year,value', file=csvfile)\n",
    "        for year in sorted(results_all[topic].keys()):\n",
    "            csvfile.write(str(year) + ',' + str(results_all[topic][year]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have planned, we have a data visualisation website that displays our data as plots over the time of different distributions of topics, words or sentiments, separated by genre or not. The visualisation is available at the end of our data story [here](http://adelamare.eu/ADA/Project/). It is also available standalone [here](http://adelamare.eu/ADA/Project/fullViz.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
